---
title: "Statistical Data Analysis - student success analyisis"
author: "Matej Ciglenečki, Petar Dragojević, Magda Radić, Tomislav Prhat"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rlang)
library(tidyverse)
library(styler)
library(fastDummies)
library(formatR) # output width cutoff
library(collections)
library(corrplot)
library(nortest) # lilliefors test
```


```{r, include=FALSE}
styler::style_dir() # Style the current file so it's well formated
message("Your current working directory is: ", getwd())
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 100), tidy = TRUE)
options(warn=-1)
```
# Introduction


# Descriptive analyisis

## Introduction
Load the data, check dimension, columns, head and summary
```{r, colapse=TRUE}
students_org <- readxl::read_excel("student_data.xlsx")
dim(students_org)
names(students_org) # column names
```

```{r, colapse=TRUE}
head(students_org[1:7], n=3) # Show first 3 rows and first 7 columns
```

```{r, colapse=TRUE}
summary(students_org[1:4]) # Show details for first 4 columns
```

Find what's the type of columns: numerical, chacaters...
```{r, colapse=TRUE}
cat("Numeric columns:", colnames(students_org %>% select(where(is.numeric))), fill=TRUE)

cat("Character columns:", colnames(students_org %>% select(where(is_character))), fill=TRUE)
# sapply(students_org, class)
```

Checking for invalid data. For example, does the data exceed maximal value specified in the dataset documentation? (it doesn't)
```{r, colapse=TRUE, results='hide'}
colMax <- students_org %>%
  select(where(is.numeric)) %>%
  sapply(., max, na.rm = TRUE)
```
Values of each column do not exceed values specified in the dataset documentation 

Removing NaN/NA/null values from the dataset. Luckily, there was no such values.
```{r, colapse=TRUE, results='hide'}
# Are there any na values?
students_org %>% filter(is.na(.))
sum(apply(students_org, 2, is.nan))
students_org %>% filter(is.null(.)) %>% summarise(n = n())

# Drop these values just in case they show up with another dataset
# We will continue using "student" variable
students <- students_org %>% filter_all(all_vars(!is.na(.) & !is.nan(.) & !is.null(.)))
students_clean <- students
```


```{r, eval=FALSE, echo=FALSE}
# Testing
# find char columns (categorical)
charcols <- names(students %>% select(where(is_character)))
charcols

students_char = students

#transform char to factors
students_char[charcols] <- lapply(students_char[charcols] , function(x) factor(x,
                                  ordered = TRUE))

# one hot encode categorical
students_dummy = dummy_cols(students_char, charcols, remove_selected_columns = TRUE)
# cor(students_dummy)
summary(lm(students_char$G3_mat ~ students_char$traveltime * students_char$higher))
```

# (In)dependence between parent's education and students success

author: Petar Dragojević - advised by the rest of the group

## Chi-squared test

https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test 

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900058/#:~:text=The%20assumptions%20of%20the%20Chi,the%20variables%20are%20mutually%20exclusive

The Chi-square statistic is a non-parametric (distribution of the data doesn't matter) test designed to analyze group differences. It's applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance.

A test of independence assesses whether observations consisting of measures on two variables, expressed in a contingency table, are independent of each other (e.g. polling responses from people of different nationalities to see if one's nationality is related to the response).

Chi-squared test assumptions:

* Sample size not less than 50 for a 2x2 contingency table - by using chi squared test on small samples, might end up committing a [Type II error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_II_error)
* Expected cell count should be 5 or more for 80% of the cells
* The observations are always assumed to be independent of each other 


First, transforming grades to the American grading system is performed:
```{r}

students <- students %>% mutate(Mat_grade =
                     case_when(G3_mat < 10 ~ "F", 
                               G3_mat >= 10 & G3_mat < 14 ~ "C",
                               G3_mat >= 14 & G3_mat < 16 ~ "B",
                               G3_mat >= 16 ~ "A")
)
students <- students %>% mutate(Por_grade =
                     case_when(G3_por < 10 ~ "F", 
                               G3_por >= 10 & G3_mat < 14 ~ "C",
                               G3_por >= 14 & G3_mat < 16 ~ "B",
                               G3_por >= 16 ~ "A")
)
```

Then father's and mother's education are grouped to the larger subgroups
```{r}
students <- students %>% mutate(MeduMod =
                     case_when(Medu == "0" | Medu == "1" | Medu == "2" ~ "0", 
                               Medu == "3" ~ "1",
                               Medu == "4" ~ "2")
)
students <- students %>% mutate(FeduMod =
                     case_when(Fedu == "0" | Medu == "1" | Fedu == "2" ~ "0", 
                               Fedu == "3" ~ "1",
                               Fedu == "4" ~ "2")
)
```


'Highest parent education' is defined as the maximum between father's and mother's education.

```{r}
students$highestparentedu <- pmax(students$MeduMod, students$FeduMod)
```

```{r, colapse=TRUE, echo=FALSE}
par(mfrow=c(1,2))

boxplot(students$G3_mat[students$MeduMod=="0"], students$G3_mat[students$MeduMod=="1"], students$G3_mat[students$MeduMod=="2"], students$G3_mat[students$FeduMod=="0"], students$G3_mat[students$FeduMod=="1"], students$G3_mat[students$FeduMod=="2"], names = c("M/low", "M/middle", "M/high", "F/low", "F/middle", "F/high"),xlab="Mother/Father education",ylab="Mathematics final grade", main="Mathemathics")

boxplot(students$G3_por[students$MeduMod=="0"], students$G3_por[students$MeduMod=="1"], students$G3_por[students$MeduMod=="2"], students$G3_por[students$FeduMod=="0"], students$G3_por[students$FeduMod=="1"], students$G3_por[students$FeduMod=="2"], names = c("M/low", "M/middle", "M/high", "F/low", "F/middle", "F/high"),xlab="Mother/Father education",ylab="Portuguese final grade", main="Portuguese")

boxplot(students$G3_mat[students$highestparentedu=="0"], students$G3_mat[students$highestparentedu=="1"], students$G3_mat[students$highestparentedu=="2"], names = c("low", "middle", "high"),xlab="Highest parent education",ylab="Mathematics final grade")

boxplot(students$G3_por[students$highestparentedu=="0"], students$G3_por[students$highestparentedu=="1"], students$G3_por[students$highestparentedu=="2"], names = c("low", "middle", "high"),xlab="Highest parent education",ylab="Portuguese final grade")
```


---

| Hypothesis | Description                                                         |
| ---------- | ------------------------------------------------------------------- |
| H0         | Mathemathics grade and highest parent education are independent     |
| H1         | Mathemathics grade and highest parent education are not independent |

```{r}
tbl = table(students$highestparentedu, students$Mat_grade)
added_margins_tbl = addmargins(tbl)
print(added_margins_tbl)
chisq.test(tbl,correct=F)$p.value
```
p-value of the test is less than 0.05. We reject the H0 hypothesis in favor of H1 hypothesis and we conclude that **education of higher educated parent and mathematics grade are dependent attributes**.

---

| Hypothesis | Description                                                  |
| ---------- | ------------------------------------------------------------ |
| H0         | mathematics grade and mother's education are independent     |
| H1         | mathematics grade and mother's education are not independent |

```{r}
tbl = table(students$MeduMod, students$Mat_grade)
added_margins_tbl = addmargins(tbl)
print(added_margins_tbl)
chisq.test(tbl,correct=F)$p.value
```
p-value of the independence test is less than 0.05. Therefore, we reject the H0 hypothesis in favor of H1 hypothesis. Conclusion is drawn that **mother's education and mathematics grade are dependent.**

---

| Hypothesis | Description                                                  |
| ---------- | ------------------------------------------------------------ |
| H0         | mathematics grade and father's education are independent     |
| H1         | mathematics grade and father's education are not independent |


```{r}
tbl2 = table(students$FeduMod, students$Mat_grade)
added_margins_tbl2 = addmargins(tbl2)
print(added_margins_tbl2)
chisq.test(tbl2,correct=F)$p.value
```
p-value of the independence test is higher than 0.05. Therefore, we do not reject the H0 hypothesis.

---

| Hypothesis | Description                                                       |
| ---------- | ----------------------------------------------------------------- |
| H0         | Portuguese grade and highest parent education are independent     |
| H1         | Portuguese grade and highest parent education are not independent |



```{r}
tbl = table(students$highestparentedu, students$Por_grade)
added_margins_tbl = addmargins(tbl)
print(added_margins_tbl)
chisq.test(tbl,correct=F)$p.value

for (col_names in colnames(added_margins_tbl)){
  for (row_names in rownames(added_margins_tbl)){
    if (!(row_names == 'Sum' | col_names == 'Sum') ){
      cat('Expected frequency for class',col_names,'-',row_names,': ',(added_margins_tbl[row_names,'Sum'] * added_margins_tbl['Sum',col_names]) / added_margins_tbl['Sum','Sum'],'\n')
    }
  }
}
```
p-value of the test is less than 0.05. We reject the H0 hypothesis in favor of H1 hypothesis and we conclude that education of higher educated parent and Portuguese grade are dependent attributes.

Expected frequency for class (grade=A, education=1) 4.391753 could be problematic for Chi-square test of independence. However, the the assumption of the test is that expected frequency should be 5 or more **in at least 80%** of the cells. In which case, Fisher's exact test should be used since it's used for smaller sample sizes.

---

| Hypothesis | Description                                                 |
| ---------- | ----------------------------------------------------------- |
| H0         | Portuguese grade and mother's education are independent     |
| H1         | Portuguese grade and mother's education are not independent |


```{r}
tbl = table(students$MeduMod, students$Por_grade)
added_margins_tbl = addmargins(tbl)
print(added_margins_tbl)
chisq.test(tbl,correct=F)$p.value
```
p-value of the independence test is less than 0.05. Therefore, we reject the H0 hypothesis in favor of H1 hypothesis. We conclude that mother's education and Portuguese grade are dependent.

---

| Hypothesis | Description                                                 |
| ---------- | ----------------------------------------------------------- |
| H0         | Portuguese grade and father's education are independent     |
| H1         | Portuguese grade and father's education are not independent |


```{r}
tbl2 = table(students$FeduMod, students$Por_grade)
added_margins_tbl2 = addmargins(tbl2)
print(added_margins_tbl2)
chisq.test(tbl2,correct=F)$p.value

for (col_names in colnames(added_margins_tbl2)){
  for (row_names in rownames(added_margins_tbl2)){
    if (!(row_names == 'Sum' | col_names == 'Sum') ){
      cat('Expected frequency for class ',col_names,'-',row_names,': ',(added_margins_tbl2[row_names,'Sum'] * added_margins_tbl2['Sum',col_names]) / added_margins_tbl2['Sum','Sum'],'\n')
    }
  }
}
```
There are 2 expected frequency whose value is less than 5. This is close to 80% of the 12 values which is why Fisher's exact test will be used in this case.

```{r}
fisher.test(tbl2)
```

p-value of the test is higher than 0.05. We do not reject the H0 hypothesis in favor of H1 hypothesis.

# Which school is better in matemathics and which in Portuguese?
author: Matej Ciglenečki - advised by the rest of the group

```{r, colapse=TRUE, eval=FALSE, echo=FALSE}
students$test <- students$absences_mat

students$test  <- students %>% mutate(quintile = ntile(test,5))%>%group_by(quintile) %>% select(quintile)
boxplot(students$G3_mat ~ unlist(students$test), names = c("QU1","QU2","QU3","QU4","QU5"),xlab="Quintiles of absences in mathematics ",ylab="Mathematics absences")
```
two (2) t-tests will be performed on four (4) different datasets. Dataset is split in four (4) different datasets (GP, MS) x (Mathematics, Portuguese): `gp_mat`, `gp_por`, `ms_mat`, `ms_por`

Mean grades for each subject will be used to decide a direction (left or right) of one-sided t-test. School with higher mean grade will be in favor of alternative H1 hypothesis.


```{r, colapse=TRUE, results='hide'}
# Show average grade for all schools
schools <- students %>%
  select("school") %>%
  distinct(.)
schools # [GP, MS]
subject_final_grade_names <- names(students)[grepl("G3", names(students))]

# all_of Note: Using an external vector in selections is ambiguous. Use `all_of(vars)` instead of `vars` to silence this message.
students_final_grade <- students %>% select("school", all_of(subject_final_grade_names))

# Select only the subject grade and school
gp_mat <- students_final_grade %>%
  filter(school == "GP") %>%
  select(G3_mat, school)
gp_por <- students_final_grade %>%
  filter(school == "GP") %>%
  select(G3_por, school)
ms_mat <- students_final_grade %>%
  filter(school == "MS") %>%
  select(G3_mat, school)
ms_por <- students_final_grade %>%
  filter(school == "MS") %>%
  select(G3_por, school)
```

Columns will be renamed for easier usage.
```{r, results='hide' }
# Rename all columns to "grade"
gp_mat <- gp_mat %>% rename(grade = G3_mat)
gp_por <- gp_por %>% rename(grade = G3_por)
ms_mat <- ms_mat %>% rename(grade = G3_mat)
ms_por <- ms_por %>% rename(grade = G3_por)
```

## Relative frequencies of subjects

Graph shows relative frequencies and means (vertical dashed lines) in mathematics grade for each school. On each graph, means are compared and school with a higher mean (vertical line to the right) is taken as an alternative to the one-sided t-test. T-test will check a statistical significance between two means. 

```{r, colapse=TRUE, echo=FALSE}
# Plot mathemathics -- final grade
ggplot(gp_mat, aes(x = grade, y = (..count.. / sum(..count..)))) +
  geom_histogram(bins = 20, aes(color = school, fill = school), alpha = 0.5) +
  geom_histogram(data = ms_mat, bins = 20, aes(color = school, fill = school), alpha = 0.3) +
  geom_vline(data = gp_mat, aes(xintercept = mean(grade), color = school), linetype = "dashed") +
  geom_vline(data = ms_mat, aes(xintercept = mean(grade), color = school), linetype = "dashed") +
  xlab("Grade") +
  ylab("Relative frequency") +
  labs(title = "Mathematics - relative frequencies and mean")
```


```{r, colapse=TRUE, echo=FALSE}
# Plot portug -- final grade
ggplot(gp_por, aes(x = grade, y = (..count.. / sum(..count..)))) +
  geom_histogram(bins = 20, aes(color = school, fill = school), alpha = 0.5) +
  geom_histogram(data = ms_por, bins = 20, aes(color = school, fill = school), alpha = 0.3) +
  geom_vline(data = gp_por, aes(xintercept = mean(grade), color = school), linetype = "dashed") +
  geom_vline(data = ms_por, aes(xintercept = mean(grade), color = school), linetype = "dashed") +
  xlab("Grade") +
  ylab("Relative frequency") +
  labs(title = "Portuguese - relative frequencies and mean")
```
On both graphs, we can see that `GP` school has higher a mean of grades in both subjects than `MS` school.

## Normality

```{r,echo=FALSE,  eval=FALSE, results='hide'}
### Ignore this block
curve(dnorm,xlim=c(-4,4)) #Normal
curve(dchisq(x,df=1),xlim=c(0,30)) #Chi-square with 1 degree of freedom

hist(colMeans(sapply(rep(10,100),rnorm)),xlab='Sample mean',main='')
hist(colMeans(sapply(rep(10,100),rchisq,df=1)),xlab='Sample mean',main='')

qqp(colMeans(sapply(rep(10,100),rnorm)),xlab='Sample mean',main='')
qqp(colMeans(sapply(rep(10,100),rchisq,df=1)),xlab='Sample mean',main='')

x <- seq(-15, 15, by=0.001)
y <- rnorm(x, mean(x), sd(x))
x2 <- rnorm(5000, -1)
y1 <- runif(30000)
y1
x2
hist(y1, breaks=50)
hist(x2, breaks=50)

hist(y, breaks=50)
ks.test(x,y,"pgamma")
lillie.test(y)
```

Normality can be check in multiple ways. In the following steps, two (2) methods are used:

* visual (`qqnorm`)
* quantitative decisions / tests (Lilliefors and Kolmogorov-Smirnov tests)
```{r, colapse=TRUE}
nrow(gp_mat) # == nrow(gp_por)
nrow(ms_mat) # == nrow(ms_por)
```
`n` - size of the dataset for mathematics is `331` and `39` for Portuguese.

```{r, colapse=TRUE, echo=FALSE}
par(mfrow=c(1,2))

qqnorm(gp_mat$grade, pch = 1, frame = FALSE, main = "GP school mathemathics")
qqline(gp_mat$grade)
qqnorm(gp_por$grade, pch = 1, frame = FALSE, main = "GP school Portuguese")
qqline(gp_por$grade)

qqnorm(ms_mat$grade, pch = 1, frame = FALSE, main = "MS school mathemathics")
qqline(ms_mat$grade)
qqnorm(ms_por$grade, pch = 1, frame = FALSE, main = "MS school Portuguese")
qqline(ms_por$grade)

```

```{r, colapse=TRUE}
lillie.test(gp_mat$grade)["p.value"]
ks.test(gp_mat$grade, "pnorm", mean(gp_mat$grade), sd(gp_mat$grade))["p.value"]


lillie.test(gp_por$grade)["p.value"]
ks.test(gp_por$grade, "pnorm", mean(gp_por$grade), sd(gp_por$grade))["p.value"]


lillie.test(ms_mat$grade)["p.value"]
ks.test(ms_mat$grade, "pnorm", mean(ms_mat$grade), sd(ms_mat$grade))["p.value"]


lillie.test(ms_por$grade)["p.value"]
ks.test(ms_por$grade, "pnorm", mean(ms_por$grade), sd(ms_por$grade))["p.value"]
```
Tails are emphasized on the left side of the distribution, which is why the `p` value will almost always be less than 0.05 for the Kolmogovor-Smirnov and Lilliefors' test. 

Visually we can see that in fact data comes from the normal distribution but with a strong remark that the left tail is often present. Although normality is assumed, tests that are sensitive to normality won't be taken into account.

## F-test of equality of variances

It's important to emphasize that F-test of equality of variances is extremely sensitive to normality. In this section, the test will be conducted but it's results and conclusions won't be taken into account. Why? Because the distribution of datasets can't be considered normal in this case (because of strong left tails).

$p\ -$ probability that under the null hypothesis of obtaining the value (of the test statistic) that's as extreme (or more extreme) than the value we got computed from the sample we have

If $p < \alpha$ hypothesis $H0$ is rejected in favor of hypothesis $H1$ 
* falls under right tail => rejection

$$H_{0}: \sigma_{1}^{2}=\sigma_{2}^{2}$$
$$H_{1}: \neg H_{0}$$

Order of arguments for the `var.test` function doesn't matter. However, in practice, the numerator has the higher value than the denominator:
$$\frac{\sigma_{1}^2}{\sigma_{2}^2},\quad\sigma_{1}^2 > \sigma_{2}^2$$


```{r, colapse=TRUE, echo=FALSE}
# This part won't be outputed as code in PDF
cat_reject_h0 <- function(prefix_message, is_h0_rejected) {
  cat(prefix_message, "\n")
  if (is_h0_rejected) cat("\tWe reject the H0 hypothesis in favor of H1 hypothesis\n") else cat("\tWe do not reject the H0 hypothesis\n")
}
```

```{r, colapse=TRUE}
cat("Mathematics variances", var(gp_mat$grade), var(ms_mat$grade))
cat("Portugeuse variances", var(gp_por$grade), var(ms_por$grade))
```

Intuitively, it can be assumed that the H0 hypothesis for Portuguese will be rejected because the variances are significantly different from each other. Of course, F-test of equality of variances has to be conducted to assure the statistical significance between the two variances.

Construction of the test:
```{r, colapse=TRUE, results='hide'}
alpha <- 0.05

# H0 - Variance of GP_MAT and MS_MAT are equal
# H1 - not H0
mat_f_test <- var.test(gp_mat$grade, ms_mat$grade, alternative = "two.sided")
mat_f_test["p.value"]

# H0 - Variance of GP_POR and MS_MAT are equal
# H1 - not H0
por_f_test <- var.test(gp_por$grade, ms_por$grade, alternative = "two.sided")
por_f_test["p.value"]
```

```{r, colapse=TRUE}
var_equal_mat <- if (mat_f_test$p.value < alpha) FALSE else TRUE
cat_reject_h0("Mathemathics - F-test of equality of variances:", !var_equal_mat)

var_equal_por <- if (por_f_test$p.value < alpha) FALSE else TRUE
cat_reject_h0("Portuguese - F-test of equality of variances:", !var_equal_por)
```


## T-test - unpaired two sample test of equal means

Because the `n` is bigger than 30 for both datasets and it true that t-test is robust to (non)normality, unpaired two sample test of equal means is conducted for both subjects.

With previously calculated means, one-sided alternative hypothesis is chosen (alternative that school `GP` has a higher mean) 

Again, because of the sensitivity to normality (F-test of equality of variances) it's assumed that variances are unequal for the t-test.

```{r, collapse=TRUE}

# H0 - GP school has equal grades to in mathematics to MS (GP=MS)
# H1 - GP>MS
mat_t_test <- t.test(gp_mat$grade, ms_mat$grade, alt = "greater", var.equal = FALSE)
is_gp_mat_better <- if (mat_t_test$p.value < alpha) TRUE else FALSE
cat_reject_h0("Mathemathics - t-test:", is_gp_mat_better)

# H0 - GP school has equal grades to in Portuguese to MS (GP=MS)
# H1 - GP>MS
por_t_test <- t.test(gp_por$grade, ms_por$grade, alt = "greater", var.equal = FALSE)
is_gp_por_better <- if (por_t_test$p.value < alpha) TRUE else FALSE
cat_reject_h0("Portugueuse - t-test:", is_gp_por_better)
```

Mathemathics - H0 hypothesis is not rejected. It can't be stated that school GP has better math grades than scchool MS

Portuguese - H0 hypothesis in favor of H1 hypothesis from which it's concluded that school `GP` has better grades in Portuguese than school `MS`.

# Are students more sucessfull in the mathematics or Portuguese?
author: Tomislav Prhat - advised by the rest of the group

```{r}
students_org  %>% summarise(
          Mean.G3_mat = mean(G3_mat),
          Mean.G3_por = mean(G3_por),
            ) -> summary.result1
summary.result1

students_org  %>% summarise(
          Med.G3_mat = median(G3_mat),
          Med.G3_por = median(G3_por),
            ) -> summary.result2
summary.result2

students_org  %>% summarise(
          Mean.G3_mat = mean(G3_mat, trim = 0.1),
          Mean.G3_por = mean(G3_por, trim = 0.1),
            ) -> summary.result3
summary.result3

(1 - summary.result3/summary.result1)*100

```

Portuguese grade's mean, median and trimmed mean (10%) is higher than mathemathic's

```{r, echo=FALSE,eval=FALSE}

students_org  %>% summarise(
          IQR.G3_mat = IQR(G3_mat),
          IQR.G3_por = IQR(G3_por),
            ) -> summary.result4
summary.result4

students_org  %>% summarise(
          Var.G3_mat = var(G3_mat),
          Var.G3_por = var(G3_por),
            ) -> summary.result5
summary.result5


students_org  %>% summarise(
          sd.G3_mat = sd(G3_mat),
          sd.G3_por = sd(G3_por),
            ) -> summary.result6
summary.result6

```

```{r, echo=FALSE}
boxplot(students_org$G3_mat, students_org$G3_por, 
        names = c('Mathemathics grade','Portugueuse grade'), ylab="Grade")
```

```{r, echo=FALSE}
hist(students_org$G3_mat, 
     breaks=seq(0, 20),
     main='Mathemathics',
     xlab='Grade')
```

```{r, echo=FALSE}
hist(students_org$G3_por, 
     breaks=seq(0, 20),
     main='Portuguese',
     xlab='Grade')
```

Before the t-test, normality is checked visually and with Kolmogorov-Smirnov's and Lilliefors' test:
```{r, echo=FALSE}
qqnorm(students_org$G3_mat, pch = 1, frame = FALSE,main='Mathemathics')
qqline(students_org$G3_mat, col = "steelblue", lwd = 2)
qqnorm(students_org$G3_por, pch = 1, frame = FALSE,main='Portuguese')
qqline(students_org$G3_por, col = "steelblue", lwd = 2)
```

```{r}
cat("Mathemathics p-value (Lillie):", unlist(lillie.test(students_org$G3_mat)["p.value"]))
cat('Mathemathics p-value (KS):', unlist(ks.test(students_org$G3_mat, "pnorm", mean(students_org$G3_mat), sd(students_org$G3_mat))["p.value"]))

cat('Portuguese p-value (Lillie):', unlist(lillie.test(students_org$G3_por)["p.value"]))
cat('Portuguese p-value (KS):', unlist(ks.test(students_org$G3_por, "pnorm", mean(students_org$G3_por), sd(students_org$G3_por))["p.value"]))

```
Small p-values are the result of left tails. Visually we can see that in fact data comes from the normal distribution but with a strong remark that the left tail is often present. Although normality is assumed, tests that are sensitive to normality won't be taken into account.

## F-test of equality of variances

Because of already mentioned extreme sensitivity to normality, theF-test of equality of variances will be conducted but it's results and conclusions won't be taken into account.

$$H_{0}: \sigma_{1}^{2}=\sigma_{2}^{2}$$
$$H_{1}: \neg H_{0}$$

```{r}
var.test(students_org$G3_mat, students_org$G3_por)
```
Because of the small p-value we reject the H0 hypothesis in favor of H1 hypothesis. Variances are different for each subject. 

## T-test for equality of grade means

In both, alternative case will be that the higher grade is in Portuguese because of higher mean value compared to mathematics.
```{r}
# H0 - Mean grades are the same (Mat=Por)
# H1 - Por > Mat
por_mat_t_test <- t.test(students_org$G3_por, students_org$G3_mat, alternative = "greater", var.equal = FALSE)

is_por_higher <- if (por_mat_t_test$p.value < alpha) TRUE else FALSE
cat_reject_h0("T-test for equality of grade means:", is_por_higher)
```



# How does travel time affect student's success?

ANOVA will be performed to answer this question.

ANOVA's assumptions are:
* independence of sample cases
* the population from which samples are drawn should be normally distributed
* homogeneity of variance (variance among the groups should be approximately equal)

H0 hypothesis - mean value of groups are equal
$$H_{0}: \mu_{1}=\mu_{2}=\ldots=\mu_{k}$$
$$H_{1}: \neg H_{0}$$

We can assume independence because the schools are different. 

If H0 hypothesis we conclude that mean values are unequal. In other words, we conclude that travel time affects the mean of student's grade (success).

## Categorical values

Groups are be defined by the attribute `traveltime`. It's necessary to transform the values from attribute `traveltime` to categorical continuous data (factors with an order). `traveltime` attribute has 4 possible values which define the travel time from school to student's home:
* `<` 15min
* 15 - 30 min
* 30 - 60 min
* `>` 60 min

Last category (60min+) will be merged with second to last category (30-60min) because only 8 data points are contained within the last group (60min+), which is significantly lower compared to the size of other groups.

```{r, colapse=TRUE}
count(students, students$traveltime)
students <- students_clean
students$traveltime <- factor(students$traveltime, ordered=TRUE, labels=c("0 - 15 min", "15 - 30 min", "> 30 min", "> 30 min"))
```

Term 'success' (G_total) is defined as sum of `G[1,2,3]_mat` i `G[1,2,3]_por` 

```{r, colapse=TRUE, results='hide'}
students$G3_total <- students$G3_mat + students$G3_por
students$G2_total <- students$G2_mat + students$G2_por
students$G1_total <- students$G1_mat + students$G1_por

students$G_por_total <- students$G1_por + students$G2_por + students$G3_por
students$G_mat_total <- students$G1_mat + students$G2_mat + students$G3_mat

students$G_total <- students$G1_total + students$G2_total + students$G3_total
```

ANOVA is robust to slight irregularities in normality. Nonetheless, normality for `G_total` will be tested for the whole dataset and then for each group independently.

```{r, colapse=TRUE, out.width="80%"}
model = lm(students$G_total ~ students$traveltime)

par(mfrow=c(1,2)) # 2 plots in 1 row

timeperiod = '0 - 15 min'
data <- rstandard(model)[students$traveltime==timeperiod]
qqnorm(data, pch = 1, frame = FALSE, main = timeperiod)
qqline(data)
hist(data, main = timeperiod)
lillie.test(data)["p.value"]
ks.test(data, 'pnorm', mean=mean(data), sd=sd(data))["p.value"]

timeperiod = '15 - 30 min'
data <- rstandard(model)[students$traveltime==timeperiod]
qqnorm(data, pch = 1, frame = FALSE, main = timeperiod)
qqline(data)
hist(data, main = timeperiod)
lillie.test(data)["p.value"]
ks.test(data, 'pnorm', mean=mean(data), sd=sd(data))["p.value"]

timeperiod = "> 30 min"
data <- rstandard(model)[students$traveltime==timeperiod]
qqnorm(data, pch = 1, frame = FALSE, main = timeperiod)
qqline(data)
hist(data, main = timeperiod)
lillie.test(data)["p.value"]
ks.test(data, 'pnorm', mean=mean(data), sd=sd(data))["p.value"]
```
On the graph, it's visible that data is normally distributed with a few outliers (left tail). `p` value of the Lilliefors' test sometimes goes below 0.05, however, it's always above 0.05 for the Kolmogorov-Smirnov test.

Lilliefors' test is used if variance and mean of the population is unknown, which is true for this dataset. It's known that Lilliefors is more conservative compared to Kolmogorov-Smirnov test, meanin that it's more likely to reject the H0 hypothesis.

Taking everything into account, normality is assumed. Deviations from normality are small and `p` values that are bellow 0.05 are relatively close to 0.05.

## Homogeneity of variance - Bartlett's test 

$$H_{0}: \sigma_{1}^{2}=\sigma_{2}^{2}=\ldots=\sigma_{k}^{2}$$
$$H_{1}: \neg H_{0}$$

```{r}
var(students$G_total[students$traveltime=="> 30 min"])
var(students$G_total[students$traveltime=='15 - 30 min'])
var(students$G_total[students$traveltime=="> 30 min"])
bartlett.test(students$G_total ~ students$traveltime)
```
Values of variances are similar. `p` value of the test is above 0.05 because of which H0 hypothesis is not rejected. With this, it's confirmed the dataset does not violate ANOVA's assumption for homogeneity of variances.

## ANOVA - Do groups have the same mean value?

$$H_{0}: \mu_{1}=\mu_{2}=\ldots=\mu_{k}$$
$$H_{1}: \neg H_{0}$$

```{r}
boxplot(students$G_total ~ students$traveltime)
```
Visually, we can assume that travel time does affect student's success. However, it's necessary to perform the ANOVA test to confirm if the difference is statistically significant.

```{r}
model = lm(students$G_total ~ students$traveltime)
anova(model)
```

ANOVA suggests that there is a difference between groups `traveltime`. Although the difference isn't enormous, the `p` (between `0.001` and `0.01`) value still suggests statistical significance. The conclusion follows: different `traveltime` groups have influence on student's success.

# Prediction of student's success with other variables
author: Magda Radić - advised by the rest of the group

First, categorical data is one-hot-encoded.

```{r, collapse=TRUE, results='hide'}
require(fastDummies)
students_org
students_dummies=dummy_cols(students_org, remove_first_dummy = TRUE, remove_selected_columns = TRUE)
students_dummies

students_dummies$G_por_total <- students_dummies$G1_por +
  students_dummies$G2_por +
  students_dummies$G3_por

students_dummies$G_mat_total <- students_dummies$G1_mat +
  students_dummies$G2_mat +
  students_dummies$G3_mat

students_dummies$G_total <- students_dummies$G_por_total +
  students_dummies$G_mat_total

```

Then, individual linear regressions are performed where `G3_mat` and `G3_por` are dependent variables and other variables are regressors. $R^2$ and p-values of the F-tests are saved to an array and will be used later to check which regressors give the minimum $R^2$ value.

```{r, collapse=TRUE}

filtered_col_names = c()
r_squares = c()
ps = c()

for(i in 1:ncol(students_dummies)){
  
  col_names=colnames(students_dummies)
  col_name=col_names[i]
  
  if (!startsWith(col_name, "G")){ # skip grades
    
    model=lm(students_dummies$G_total ~ students_dummies[[col_name]])
    
    summary_model =summary(model)
    
    # appending values
    filtered_col_names <- append(filtered_col_names, col_name)
    
    r_squares <- append(r_squares, summary_model$r.squared)
    ps <- append (ps,  pf(summary_model$fstatistic[1], summary_model$fstatistic[2], summary_model$fstatistic[3], lower.tail=FALSE))
  }
}

df_g_squares=data.frame(filtered_col_names, r_squares, ps)

head(df_g_squares, n=3)
```

## Predicting mathemathics success

Which variables are the best predictors for `G_total`? Predictors are sorted by coefficient of determination $R^2 \in [0,1]$ a statistical measure that represents the proportion of the variance for a dependent variable (`G_total`) that's explained by an independent variable or variables in a regression model.

In this case, 10 best predictors are taken into a consideration (ranked by the $R^2$ value). After which, 

```{R, collapse=TRUE}
df_top_predictors = df_g_squares[order(-df_g_squares$r_squares[1:10]), ]
top_10_predictors = as.vector(df_top_predictors$filtered_col_names)
top_10_predictors
```

From top 10 predictors it might be desirable to ditch one of the predictors which is highly correlated with another as they describe similar variability. Decision will be performed with visual and quantitative review of the correlation matrix. If there is any pair of predictors whose absolute correlation value is higher than 0.7, one of the predictors from the pair will be ditched. Preferably, it would be a predictor whose sum of absolute correlations coefficients with other predictors is higher than  
```{r, collapse=TRUE}
df_student_success <- students_dummies[,top_10_predictors]
corrplot(cor(df_student_success), addCoef.col = 'black', number.cex = 0.5)
```

All predictors will stay in the consideration since there isn't a pair of predictors whose absolute correlation value exceeds `0.7`. 

```{r, collapse=TRUE}
model_top_pred <- lm(students_dummies$G_total ~ . ,df_student_success)
summary_top_pred <- summary(model_top_pred) 
summary_top_pred


ps_top_pred <- summary_top_pred$coefficients[,4]
ps_top_pred[order(ps_top_pred)]  # $coefficients[,4] -> p-values
```
Regressors that explain the most amount of variance within the student's success are times of failure for a subject. Those regressors might be a consequence of bad grades, not the cause. More non-obvious regressors are mother's education, frequency of going out (haning out with friends) and amount of time spent studying.

Model will be further simplified so that it doesn't last two (2) regressors with highest p-values. Higher p-value indicate lower explanation of variance.
```{r, collapse=TRUE}
top_pred_trim <- top_10_predictors[1 : (length(top_10_predictors) - 2)]
df_student_success_trim <- df_student_success[,top_pred_trim]

model_top_pred_trim <- lm(students_dummies$G_total ~ . ,df_student_success_trim)
summary_top_pred_trim <- summary(model_top_pred_trim)
summary_top_pred_trim

ps_top_pred_trim <- summary_top_pred_trim$coefficients[,4]  # $coefficients[,4] -> p-values
ps_top_pred_trim[order(ps_top_pred_trim)] 
```

The $R^2$ is smaller, however, the adjusted $R^2$ is higher compared the previous model which indicates that unnecessary regressors were discarded. This linear model represents the proportion of the variance (22.25%) for a dependent variable (`G_total`) that's explained by top 8 variables.

## Normality of residuals

https://analyse-it.com/docs/user-guide/fit-model/linear/residual-normality#:~:text=Normality%20is%20the%20assumption%20that,normally%20distributed%2C%20or%20approximately%20so.&text=If%20the%20test%20p%2Dvalue,not%20from%20a%20normal%20distribution.

> Violation of the normality of residuals assumption only becomes an issue with small sample sizes. For large sample sizes, the assumption is less important due to the central limit theorem, and the fact that the F and t-tests used for hypothesis tests and forming confidence intervals are robust to modest departures from normality.

On graphs, it's visible that residuals are normally distributed.

```{r, collapse=TRUE}
hist(rstandard(model_top_pred_trim))
qqnorm(rstandard(model_top_pred_trim))
ks.test(rstandard(model_top_pred_trim),'pnorm')
```